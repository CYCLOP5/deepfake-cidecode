# DeepFake Detection Pipeline – Technical Overview

This project implements a multi-modal DeepFake detection pipeline that processes video, image, and audio inputs to determine if the media has been manipulated. The system uses advanced deep learning techniques for visual analysis and classical machine learning for audio analysis, integrating two parallel detection methods: a plain frames-based method and an MRI-GAN based method. Below is an in-depth technical explanation of the pipeline's logic.

---

## 1. Input Handling and Preprocessing

### 1.1. Media Input Types
- **Video:**  
  - Frames are extracted at regular intervals (e.g., every 10th frame) to reduce computational load.
- **Image:**  
  - Static images are converted into short video clips by repeating the image for a set duration (e.g., 3 seconds at 30 fps).
- **Audio:**  
  - Audio files are processed using Librosa:
    - **Loading & Trimming:** Audio is loaded at a target sample rate (e.g., 16 kHz) and trimmed of silence using `librosa.effects.trim`.
    - **Normalization & Length Adjustment:** The waveform is normalized (dividing by its maximum absolute value) and padded or truncated to a fixed length (e.g., 32000 samples).
    - **Feature Extraction:** A mel-spectrogram is computed with 128 mel bands, converted to decibels, and then flattened to serve as input for a logistic regression model.

### 1.2. Face Detection and Extraction
- **Face Detection:**  
  - A pre-trained face detection module (e.g., MTCNN) is employed to detect and extract facial landmarks from each frame.
  - This process crops facial regions to ensure subsequent processing focuses on relevant areas.

- **Data Augmentation:**  
  - During training, augmentation techniques (rotation, scaling, flipping, distractor overlays) are applied to simulate real-world variations and improve model robustness.

---

## 2. DeepFake Detection Models

The pipeline implements two parallel methods to detect manipulated faces:

### 2.1. Plain Frames-based Detection
- **Model Architecture:**  
  - Utilizes Efficient-Net B0, pre-trained on ImageNet and fine-tuned for binary classification (Real vs. Fake).
- **Processing Flow:**  
  - Each detected face is resized to 224×224, normalized with standard mean and standard deviation values, and directly fed into the classifier.
- **Grad-CAM Integration:**  
  - Optionally, Grad-CAM is applied to generate activation heatmaps, providing visual explanations for the classification decisions.

### 2.2. MRI-GAN Based Detection
- **MRI-GAN Architecture:**  
  - **Generator:**  
    - Based on a U-Net architecture with 16 layers, featuring skip connections between encoder and decoder layers.
    - Down-sampling modules use convolution, instance normalization, LeakyReLU, and dropout; up-sampling modules employ transposed convolutions, instance normalization, and ReLU.
  - **Discriminator:**  
    - Implements a PatchGAN architecture that focuses on local image patches, thereby enforcing high-frequency detail accuracy.
- **Loss Functions:**  
  - **Conditional GAN Loss:** Encourages the generator to produce realistic MRI outputs conditioned on the input face.
  - **Pixel-wise L2 Loss:** Measures the mean squared error between the generated MRI and the ground truth MRI.
  - **Perceptual (SSIM-based) Loss:**  
    - Uses a modified Structural Similarity Index Metric (SSIM) to assess perceptual differences.
    - Defined as:  
      ```
      L_per(G) = √(1 - SSIM(x, y))
      ```
- **Loss Aggregation:**  
  - The total generator loss is computed as:
    ```
    L(G) = L_cGAN(G, D) + λ (τ * L_L2(G) + (1 - τ) * L_per(G))
    ```
    where λ is a scaling factor (set to 100) and τ is a hyperparameter that balances the contribution of the pixel-wise L2 loss and the perceptual loss.
- **Hyperparameter τ:**  
  - In our implementation, **τ is set to 0.15** (instead of 0.3), which places more emphasis on the perceptual loss component during MRI generation. This adjustment has been empirically determined to improve the sensitivity of the MRI outputs in capturing subtle manipulations.
- **Classification on MRI Images:**  
  - The MRI images generated by the GAN are passed through a dedicated Efficient-Net B0 classifier to produce a DeepFake prediction.

---

## 3. Aggregation and Final Decision

- **Face-Level Aggregation:**  
  - For each video, predictions from every detected face (from both the plain frames and MRI-GAN branches) are aggregated.
- **Fake Fraction Threshold:**  
  - A predetermined fake fraction threshold is used to determine if the entire video is manipulated.
- **Audio Integration:**  
  - If audio is present, its prediction (obtained via a logistic regression model) is incorporated into the final decision.
- **Output Generation:**  
  - The aggregated results, including prediction probabilities, Grad-CAM visualizations, and file locations of processed outputs, are logged and saved as a JSON report.

---

## 4. End-to-End Workflow

1. **Input File Identification:**  
   - Determine if the input is a video, image, or audio file based on the file extension.
2. **Preprocessing:**  
   - Video: Extract frames and detect faces.
   - Image: Convert to video, then extract frames and faces.
   - Audio: Process to extract mel-spectrogram features.
3. **Parallel Detection:**  
   - **Plain Frames Method:** Direct classification of faces using Efficient-Net B0.
   - **MRI-GAN Method:** Generate MRI images from faces using a GAN (with τ = 0.15) and classify them using a dedicated Efficient-Net B0.
   - **Audio Analysis:** Process audio features through a logistic regression model.
4. **Aggregation:**  
   - Combine face-level predictions using a fake fraction threshold and merge with audio predictions to form a multi-modal decision.
5. **Result Output:**  
   - The final DeepFake decision and detailed logs are outputted in a JSON file.

---

This technical overview details the architecture, loss functions, and hyperparameters (with τ set to 0.15) used in the MRI-GAN model, as well as the complete workflow of the multi-modal DeepFake detection pipeline.



## Credits

This project builds upon the foundational concepts introduced in the MRI-GAN model for DeepFake detection by Pratikkumar Prajapati and Chris Pollett. While inspired by their work, we have made several adaptations and optimizations to enhance functionality and tailor the implementation to our specific use case.
```
@misc{2203.00108,
Author = {Pratikkumar Prajapati and Chris Pollett},
Title = {MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment},
Year = {2022},
Eprint = {arXiv:2203.00108},
}
